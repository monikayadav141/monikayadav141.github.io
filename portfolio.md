
Research Portfolio

1. Principal Investigator, Large-Scale Misinformation Field Experiment

Methods: Field experiment, randomized controlled trial, data analysis, structural modeling
Core Skills: End-to-end research management, experiment design, survey design, causal inference, stakeholder communication, team leadership

Description:
•	Designed and developed a mobile fact-checking app that delivered digest-style credibility prompts, experimenting with content formats and delivery cadences that maximized user comprehension and reduced cognitive load.
•	Conducted iterative cycles of app optimization with designers and developers, reducing user confusion and drop-off and yielding more reliable behavioral data.
•	Deployed the app in a large multi-wave randomized field experiment in India to test whether repeated exposure to fact-checking digests improved users’ ability to identify misinformation.
•	Secured a $30K grant and managed end-to-end project operations, including user recruitment, training of research interns, coordination of weekly deployments, and oversight of complex field logistics.
•	Cleaned and analyzed large multi-wave interaction datasets with high attention to data quality, identifying the heuristics users relied on when evaluating content credibility.
•	Demonstrated measurable improvements in participants’ truth discernment, showing that the intervention helped users more accurately evaluate online information over time.
•	Applied structural modeling to separate genuine gains in truth discernment from generalized skepticism, increasing confidence in the interpretation of treatment effects.
•	Produced clear, stakeholder-facing insight reports that translated complex causal findings into actionable guidance, informing ongoing product and policy discussions on misinformation education tools.

2. Qualitative Researcher, Narratives Project 
Methods: 100+ hours participant observation, 40+ semi-structured interviews, focus groups, content analysis, thematic coding

Core Skills: Generative qualitative research, deep contextual inquiry, conversational analysis, behavioral insights, thematic synthesis

Description: 

•	Led a multi-site qualitative research study to understand how people interpret, share, and react to information in real-world settings, providing deep user insights that informed more resonant communication and product messaging strategies.
•	Conducted interviews, focus groups, and contextual observation to identify how users make sense of information, form judgments, and build or lose trust, revealing core behavioral drivers that shape engagement and decision-making.
•	Mapped emotional, contextual, and social cues that shape user interpretation to diagnose where messages failed or misaligned with user expectations, leading to clearer design recommendations for content framing and information delivery.
•	Developed a structured framework for narrative activation, resonance, and resistance to categorize how different user groups process and respond to content, supporting more targeted and effective communication strategies.
•	Communicated insights to interdisciplinary teams and translated findings into opportunity areas and trust-building behavioral interventions.

3. Lead Survey Researcher, Conjoint Survey Experiment
Methods: Original survey experiment, multi-round conjoint design, treatment manipulation, survey analysis, AMCE estimation

Core Skills: Experiment design, survey UX design, Qualtrics programming (JS/CSS), quantitative analysis, iterative testing, modeling learning/feedback loops
Description: 
•	Designed and implemented an advanced multi-round survey experiment with dynamic feedback loops to test how accountability shapes resource-allocation decisions.
•	Built and refined a complex conjoint task with real-time scorecards to improve participants’ understanding of scenarios and task flow, leading to higher data quality and clearer behavioral signals.
•	Programmed custom JavaScript logic in Qualtrics to support dynamic task sequencing and scenario updates, improving interaction flow and reducing user confusion.
•	Conducted iterative UX testing of the conjoint tasks to identify and address friction points in information density and interaction sequences, increasing completion rates and producing more interpretable responses.
•	Developed a pre-analysis plan and validated hypotheses with pilot data to ensure analytic transparency and strengthen causal interpretation, resulting in a more rigorous and defensible research design.
•	Cleaned and analyzed multi-round user-level data using AMCE and interaction models to isolate the attributes driving user choices, yielding precise estimates of tradeoffs and decision heuristics.
•	Translated complex modeling results into concise, stakeholder-ready insights.





4. Quantitative Researcher, Labor & Litigation Project 
Methods: Original dataset construction, scraping and integrating legal records, data wrangling, statistical modeling

Core Skills: Data engineering, quantitative causal analysis, longitudinal modeling, working with messy datasets, deriving insights from large administrative data

Description:
•	Built a 25-year, multi-source dataset from fragmented legal and administrative records, enabling reliable, large-scale analysis of user decision-making patterns.
•	Transformed tens of thousands of unstructured court filings into clean, interpretable data, uncovering how institutional ecosystems influence individual choices.
•	Developed automated data-wrangling workflows that streamlined processing and reduced error in handling large-scale administrative data.
•	Identified statistically significant behavioral patterns using longitudinal modeling, generating insights that informed discussions about user incentives and ecosystem-level friction.
•	End-to-end ownership of a large quantitative research project, from data collection and cleaning to modeling and insight translation for non-technical audiences.

5. Teaching Assessment Fellow, Columbia Center for Teaching and Learning
Methods: Program evaluation, mixed-methods assessment (quant + qual), semi-structured interviews, survey analysis, feasibility study, AI tool evaluation

Core Skills: Evaluation research, impact analysis, generative AI assessment, qualitative & quantitative triangulation, translating research into recommendations, identifying user needs across diverse groups, developing evidence-based recommendations, communicating insights to institutional decision-makers.

Description: 

•	Designed and led mixed-methods evaluation frameworks for new instructional interventions and CTL’s flagship programs to understand how users engaged with learning tools and teaching workflows, leading to evidence-based recommendations adopted by faculty and CTL leadership.
•	Evaluated an AI-generated learning-plan tool in a high-stakes nursing course to identify usability pain points, informing leadership’s decision to delay rollout and refine the AI system.
•	Conducted statistical analysis of exam performance and qualitative analysis of student feedback to assess whether instructor-designed interventions improved learning, guiding future instructional design.
•	Analyzed survey and interview data from the Graduate Student Teaching Orientation program to surface engagement gaps and unmet user needs across disciplines, resulting in targeted redesigns to improve clarity, reach, and usability, especially for STEM instructors.
•	Mapped barriers to adoption across diverse instructor groups to diagnose where teaching programs failed to align with user expectations and workflows, co-developing program updates that improved usability and participation.
•	Synthesized complex quantitative and qualitative findings into concise internal briefs to provide actionable guidance for cross-functional stakeholders, enabling informed decisions about AI adoption, program strategy, and future investments.


6.	Ethnographic Research Lead, Worker Peer-Learning Project

Methods: Ethnographic fieldwork, 40+ semi-structured interviews, 20 weeks of participant observation, multilingual fieldwork, qualitative coding

Core Skills: Deep qualitative inquiry, contextual research, managing complex stakeholder environments, cultural insights, behavioral analysis
Description:
•	Conducted in-depth contextual inquiry in a high-stakes, multi-stakeholder environment to understand how workers learned, collaborated, and managed risk, uncovering unmet user needs and improving training efficiency and safety practices.
•	Mapped end-to-end workflows at a complex worksite to spot friction in onboarding and peer-led training, highlighting hidden efficiency gaps and informing redesigned training steps that improved coordination and reduced rework.
•	Synthesized unstructured field data into a systems map of worker skills and dependencies, identifying sources of errors and bottlenecks and helping supervisors standardize informal training for smoother task handoffs and fewer quality issues.
•	Identified key usability issues in on-site workflows such as task sequencing, tool use, and supervision loops and developed evidence-based recommendations that created a more efficient, structured onboarding path for new workers.


