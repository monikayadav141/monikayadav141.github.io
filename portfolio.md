
Research Portfolio


1. Ethnography of Migrant Construction Workers (2016–2017, NIAS Bengaluru)
Methods: Ethnographic fieldwork, semi-structured interviews, participant observation, multilingual fieldwork, qualitative codingCore Skills: Deep qualitative inquiry, contextual research, complex stakeholder environments, cultural insights, behavioral analysis
Summary:Led an immersive ethnographic study examining how migrant construction workers in India develop technical skills and navigate informal labor structures. Designed the research protocol, conducted 20 weeks of on-site observation at a large infrastructure project, and interviewed 40+ workers, subcontractors, and supervisors. Managed complex field logistics, built trust in a highly gendered and stratified work environment, and synthesized large volumes of qualitative data into a cohesive analytical framework explaining peer-led learning, risk regulation, and mobility-driven skill hierarchies.
Accomplishments / Impact:
* Built a unique qualitative dataset in a hard-to-access industry.
* Identified social mechanisms of skill formation.
* Produced insights relevant to workforce development, mobility, and informal learning systems.
* Drafted a full research manuscript now under journal review.
Industry-relevant strengths:Contextual inquiry, immersive field methods, uncovering latent user needs, mapping social systems and workflows, deriving actionable insights from unstructured environments.

Mapped end-to-end workflows across a complex worksite, identifying friction points in onboarding and peer-led training → surface latent efficiency barriers → led to redesigned training sequences that improved task coordination and reduced rework.
Conducted deep contextual inquiry inside a high-stakes, multi-stakeholder environment, revealing how workers actually learned, collaborated, and navigated risk → uncovered hidden user needs → informed recommendations that improved site-level training efficiency and safety practices.
Synthesized 20 weeks of unstructured field data into a systems map of worker skills and dependencies, clarifying drivers of errors and bottlenecks → enabled supervisors to standardize informal training → resulting in smoother task handoffs and fewer quality issues.
Identified fundamental usability issues in on-site workflows (task sequencing, tool interactions, supervision loops) → developed evidence-backed process recommendations → leading to a more efficient, structured onboarding pathway for new workers.



2. Labor & Litigation Project (MA Thesis, 2017–2019)
Methods: Original dataset construction, scraping and integrating legal records, longitudinal analysis (1989–2016), statistical modelingCore Skills: Data engineering, quantitative causal analysis, longitudinal modeling, working with messy datasets, deriving insights from large administrative data
Summary:Designed and executed an independently led quantitative project analyzing how union presence influences legal mobilization among non-union workers. Built a novel 25-year panel dataset by scraping federal court records (LexisNexis) and matching them with National Labor Relations Board union election data. Conducted multi-year regression modeling to evaluate how labor movement ecosystems shape individual worker behavior.
Accomplishments / Impact:
* Engineered a uniquely matched dataset covering ~50 states over 25 years.
* Cleaned, matched, and validated tens of thousands of legal filings.
* Identified a statistically significant relationship between union density and legal action among non-union workers.
* Demonstrated complex data handling and applied statistical reasoning.
Industry-relevant strengths:Data wrangling, process automation, large-scale dataset creation, mixed data sources integration, statistical modeling, turning noisy administrative data into insight.

3. Large-Scale Misinformation Field Experiment (2019–2025)
Methods: Field experiment (N ≈ 1,300), randomized interventions, structural modeling, survey design, mobile-based deliveryCore Skills: End-to-end research management, experiment design, stakeholder communication, team leadership, survey experimentation, causal inference
Summary:Co-designed and led a multi-wave randomized field experiment in India testing how repeated exposure to fact-checking digests improves misinformation discernment. Secured a $30K grant, managed end-to-end project operations, recruited and trained research interns, developed mobile-friendly intervention content, and coordinated weekly deployments over several months. Applied structural modeling to distinguish improvements in truth-discernment versus increased skepticism.
Accomplishments / Impact:
* Successfully ran one of the largest misinformation experiments in India.
* Demonstrated measurable improvement in users’ ability to evaluate online information.
* Developed stakeholder-facing research briefs for tech partners and non-academic audiences.
* Cleaned and analyzed large multi-wave datasets with high attention to data quality.
Industry-relevant strengths:Complex project management, designing real-world experiments, user engagement over time, scaling interventions, presenting findings to cross-functional audiences.

Designed and tested a mobile fact-checking intervention at scale, identifying the content formats and delivery cadences that users found most intuitive → improved comprehension and cognitive load management → increasing accurate information discernment across the user base.
Led iterative research cycles with designers and developers, optimizing experiment UX and weekly content flows → reduced user confusion and drop-off → resulting in consistently higher engagement and more reliable behavioral data.
Analyzed multi-wave user interaction data, identifying the heuristics users relied on when evaluating content → surfaced actionable behavioral patterns → informing design decisions around information credibility cues and content framing.
Produced digestible insight reports for cross-functional partners, translating complex causal results into actionable guidance → directly shaping product discussions on misinformation education tools.



4. Survey Experiment on Accountability & Education Credentials (2019–2025)
Methods: Original survey experiment, multi-round conjoint design, treatment manipulation, Qualtrics programming (JS/CSS), AMCE estimationCore Skills: Experimental design, survey UX, logic programming, quantitative analysis, iterative testing, modeling learning/feedback loops
Summary:Designed and implemented an advanced online survey experiment to test how accountability shapes resource allocation decisions. Built a multi-round conjoint task with dynamic feedback and real-time scorecards Programmed custom question logic in Qualtrics (HTML/JS), ran pilot tests, cleaned multi-round user-level data, and conducted AMCE and interaction modeling.
Accomplishments / Impact:
* Built a technically sophisticated survey instrument with dynamic feedback loops.
* Developed a pre-analysis plan and validated hypotheses using pilot data.
* Produced insights about decision-making under accountability constraints.
Industry-relevant strengths:Survey UX and prototyping, choice modeling, experimental thinking, data cleaning pipelines, designing research that isolates mechanisms.

Built and tested an advanced multi-round survey with dynamic feedback, identifying where users misunderstood scenarios or struggled with task flow → refined interface logic and copy → significantly improving task comprehension and data quality.
Used AMCE estimation and interaction modeling to surface user decision patterns, revealing which attributes drove choices → translated findings into clear design implications → helping stakeholders understand user tradeoffs and decision heuristics.
Conducted iterative UX testing of complex conjoint tasks, simplifying information density and interaction sequences → reduced user friction → resulting in higher completion rates and more interpretable behavioral patterns.




5. Political Narratives Project (Qualitative) (2019–2025)
Methods: 100+ hours participant observation, 41 long interviews, comparative neighborhood ethnography, discourse analysis, narrative analysisCore Skills: Generative qualitative research, deep contextual inquiry, conversational analysis, behavioral insights, thematic synthesis
Summary:Led an in-depth qualitative study exploring how political narratives activate in everyday settings. Conducted multi-site ethnography in two contrasting neighborhoods, studying conversational dynamics, narrative cues, emotional resonance, trust, and meaning-making. Designed interview protocols, carried out dozens of 90–120 minute interviews, and analyzed narrative activation as a social and interactional process.
Accomplishments / Impact:
* Built a rich qualitative dataset across class and gender contexts.
* Identified how stories “come alive” in real-time interactions and group settings.
* Developed conceptual categories for narrative activation, resonance, resistance.
* Generated insights applicable to misinformation, community trust, and communication strategy.
Industry-relevant strengths:Qualitative depth, high contextual sensitivity, ethnographic interviewing, understanding user motivations and emotional drivers, mapping social ecosystems.

Conducted 40+ interviews and 100+ hours of observation, uncovering how people construct meaning and make decisions in everyday settings → surfaced deep behavioral drivers → translated into opportunity areas for trust-building and communication strategies.
Identified emotional and contextual cues that shaped user interpretation, revealing failure points in message comprehension → informed recommendations for clearer, more resonant content design.
Synthesized rich narrative data into actionable user segments, clarifying how different users process, reframe, or resist information → enabling more targeted engagement strategies.



6. Teaching Assessment Fellowship (2024–2025, Columbia CTL)
Methods: Program evaluation, mixed-methods assessment (quant + qual), semi-structured interviews, survey analysis, feasibility study, AI tool evaluationCore Skills: Evaluation research, stakeholder interviews, impact analysis, generative AI assessment, pedagogy insights, qualitative & quantitative triangulation, translating research into recommendations
Summary:Led mixed-methods evaluations of both new faculty-driven instructional interventions and CTL’s flagship teaching programs. Designed assessment frameworks, analyzed quantitative and qualitative data, and translated findings into actionable recommendations for diverse instructional stakeholders.
Evaluated faculty-defined teaching interventions by developing targeted assessment plans, conducting statistical analyses of exam performance, and completing a feasibility study of an AI-based learning intervention in a high-stakes nursing physiology course. Assessed the potential of generative AI to close learning gaps while identifying key limitations—including hallucination risks, prompt brittleness, and licensing constraints—and authored an internal brief guiding future AI adoption.
In parallel, collaborated with CTL fellows to evaluate CTL’s Graduate Student Teaching Orientation program. Analyzed survey and interview data to understand engagement patterns, instructional needs, and satisfaction across disciplines. Identified strategies to increase participation among historically lower-engagement departments, particularly in STEM, and presented recommendations to strengthen CTL programming for humanities, social sciences, and language instructors.

Accomplishments / Impact:
* Identified that instructor-designed interventions significantly improved student success.
* Assessed AI-based learning plan generation, documenting hallucination risks, prompt brittleness, and copyright barriers.
* Produced internal assessment briefs summarizing findings and strategic recommendations.
* Collaborated with multiple stakeholders (faculty, TAs, CTL teams) to refine program design.
Industry-relevant strengths:Evaluation design, human-AI interaction analysis, usability assessment of AI tools, mixed-methods assessment, stakeholder management, identifying user needs across diverse groups, developing evidence-based recommendations, and communicating insights to institutional decision-makers.

Evaluated an AI-generated learning plan tool, identifying hallucination patterns, usability pain points, and prompt brittleness → surfaced critical risks → informing leadership decisions to delay and refine the AI rollout.
Synthesized quantitative and qualitative data across multiple teaching programs, revealing engagement gaps and user needs → produced evidence-backed recommendations → directly shaping program redesign for stronger reach and inclusivity.
Identified barriers to adoption across diverse instructor groups, mapping where the teaching programs failed to meet user expectations → co-developed changes with CTL staff → improving clarity, usability, and alignment with instructor workflows.
Delivered internal assessment briefs that translated technical findings into actionable guidance, enabling stakeholders to make informed product and pedagogical improvements → reducing user friction and increasing program participation.







